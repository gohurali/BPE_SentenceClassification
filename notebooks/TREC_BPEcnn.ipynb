{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TorchImplementation_SkillClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7jPEDTYTl5g",
        "colab_type": "code",
        "outputId": "620007bc-aa80-4f20-8c4d-f6ce28d15eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08mwBGRYmmLL",
        "colab_type": "code",
        "outputId": "05e67d6f-eee8-429f-ef99-273b6de3c832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install bpemb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.28.1)\n",
            "Collecting sentencepiece (from bpemb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.21.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.8.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.3.1)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.9.205)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.205 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.12.205)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.205->boto3->smart-open>=1.2.1->gensim->bpemb) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.205->boto3->smart-open>=1.2.1->gensim->bpemb) (0.14)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.0 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "754MvFTwTyu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61448194-ce91-410b-8c05-8d25b93e3b76"
      },
      "source": [
        "__author__ = 'Gohur Ali'\n",
        "import numpy as np\n",
        "import os               # FileSystem Access\n",
        "import yaml             # Config File Access\n",
        "from tqdm import tqdm   # Progress Visualization\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "import codecs\n",
        "from bpemb import BPEmb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pJdIuFdXb3A",
        "colab_type": "code",
        "outputId": "ec5a6fb3-f130-4587-815f-c28537cf55a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUp5MszVNWhD",
        "colab_type": "code",
        "outputId": "fdda089e-8287-446b-ad1f-07599ed9700c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# CUDA for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device state:\\t\", device)\n",
        "print(\"Device index:\\t\",torch.cuda.current_device())\n",
        "print(\"Current device:\\t\", torch.cuda.get_device_name(device))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device state:\t cuda\n",
            "Device index:\t 0\n",
            "Current device:\t Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1AnYXG9T-SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShallowCNN(torch.nn.Module):\n",
        "    def __init__(self, pretrained_embeddings):\n",
        "        super(ShallowCNN,self).__init__()\n",
        "        self.cfg = yaml.safe_load(open('/content/drive/My Drive/College/Undergraduate Research/SkillEvaluation/config_torch.yaml'))\n",
        "        \n",
        "        # -- Build Embedding Table --\n",
        "        self.pretrained_embedding_table = torch.nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "        \n",
        "#         self.pretrained_embedding_table = torch.nn.Embedding(\n",
        "#                        num_embeddings=len(pretrained_embeddings), \n",
        "#                        embedding_dim=self.cfg['embedding_dim']\n",
        "#         )\n",
        "#         self.pretrained_embedding_table.weight = torch.nn.Parameter(pretrained_embeddings)\n",
        "        \n",
        "        # -- Define Architecture --\n",
        "        self.conv1 = torch.nn.Conv1d(in_channels=self.cfg['pad_limit'],\n",
        "                                     out_channels=400,\n",
        "                                     kernel_size=(4,),\n",
        "                                     stride=1,\n",
        "                                     padding=0,\n",
        "                                     bias=True\n",
        "                                    )\n",
        "        self.mp1 = torch.nn.MaxPool1d(kernel_size=2,\n",
        "                                      stride=1,\n",
        "                                      padding=0\n",
        "                                     )\n",
        "        self.fc1 = torch.nn.Linear(in_features=118400,#38400,#self.cfg['embedding_dim'] - 4,\n",
        "                                       out_features=128, \n",
        "                                       bias=True\n",
        "                                      )\n",
        "        if(self.cfg['if_softmax']):\n",
        "            self.fc2 = torch.nn.Linear(in_features=128,\n",
        "                                       out_features=6,\n",
        "                                       bias=True\n",
        "                                      )\n",
        "        else:\n",
        "            self.fc2 = torch.nn.Linear(in_features=128,\n",
        "                                       out_features=1,\n",
        "                                       bias=True\n",
        "                                      )\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass definition\n",
        "        Args:\n",
        "            inputs - Array of indices for embeddings lookup\n",
        "        \"\"\"\n",
        "        emb = self.pretrained_embedding_table(inputs)\n",
        "        x = F.leaky_relu(self.conv1(emb))\n",
        "        x = self.mp1(x)\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        #print('Flatten = ', x.shape)\n",
        "        \n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        if(self.cfg['if_softmax']):\n",
        "            x = self.fc2(x)\n",
        "            x = F.log_softmax(x,dim=1,dtype=torch.float)\n",
        "        return x\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIU1KBRLpm4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        self.cfg = yaml.safe_load(open('/content/drive/My Drive/College/Undergraduate Research/SkillEvaluation/config_torch.yaml'))\n",
        "        # Used pre-determined data split\n",
        "        self.use_default_split = True\n",
        "        \n",
        "        if(self.use_default_split):\n",
        "            self.x_train,self.y_train,self.x_test,self.y_test = self.get_trec_dataset(self.cfg['train_data_location'], use_default_split=self.use_default_split)\n",
        "                      \n",
        "            self.bpe_model, self.embeddings = self.open_bpe_vectors()\n",
        "\n",
        "            self.x_train = self.bpe_model.encode_ids(self.x_train)\n",
        "            self.x_test = self.bpe_model.encode_ids(self.x_test)\n",
        "            self.x_train = pad_sequences(sequences=self.x_train,maxlen=self.cfg['pad_limit'])\n",
        "            self.x_test = pad_sequences(sequences=self.x_test, maxlen=self.cfg['pad_limit'])         \n",
        "                        \n",
        "            self.train_idx_labels = self.y_train\n",
        "            self.test_idx_labels = self.y_test\n",
        "            \n",
        "        if(self.cfg['if_softmax']):\n",
        "            self.y_train = self.to_categorical(self.y_train, self.cfg['num_classes'])\n",
        "            self.y_test = self.to_categorical(self.y_test, self.cfg['num_classes'])\n",
        "        else:\n",
        "            self.examples, self.labels = self.get_trec_dataset(self.cfg['train_data_location'], use_default_split=self.use_default_split)\n",
        "            self.examples = self.sequence_examples(self.examples)\n",
        "        \n",
        "        print('Train data size: x_train = {',self.x_train.shape,'} -- y_train = {',self.y_train.shape,'}')\n",
        "        print('Test data size: x_test = {',self.x_test.shape,'} -- y_test = {',self.y_test.shape,'}')\n",
        "\n",
        "        self.train_dataloader,self.test_dataloader = self.create_dataloaders(train_data=(self.x_train,self.y_train),\n",
        "                                                                             test_data=(self.x_test,self.y_test)\n",
        "                                                                             )\n",
        "        pass\n",
        "        \n",
        "    def sequence_examples(self, dataset):\n",
        "        sequenced_dataset = []\n",
        "        for example in tqdm(dataset):\n",
        "            sequenced_sentence = []\n",
        "            words = example.split()\n",
        "            for word in words:\n",
        "                if(word in self.w2e.keys()):\n",
        "                    idx = self.w2e[word][0]\n",
        "                    sequenced_sentence.append(idx)\n",
        "                else:\n",
        "                    idx = self.w2e['_unk'][0]\n",
        "                    sequenced_sentence.append(idx)\n",
        "            sequenced_dataset.append(sequenced_sentence)\n",
        "        return sequenced_dataset\n",
        "      \n",
        "    def create_dataloader(self, features, labels):\n",
        "        print('-- Batch size ',self.cfg['batch_size'],'--')\n",
        "        dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.cfg['batch_size'], shuffle=True)\n",
        "        return data_loader\n",
        "    \n",
        "    def to_categorical(self, y, num_classes):\n",
        "        \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "        return np.eye(num_classes, dtype='uint8')[y]              \n",
        "        \n",
        "    def get_trec_dataset(self, train_data_location, use_default_split=False):\n",
        "        \"\"\"Open and prepare the subjectivity dataset. Using\n",
        "        Regular expressions to clean the sentences.\n",
        "        \n",
        "        Args:\n",
        "            train_data_location - location of the data, specified in config\n",
        "        Return:\n",
        "            dataset - ndarray of each example\n",
        "            labels - array of binary labels\n",
        "        \"\"\"\n",
        "        \n",
        "        if(use_default_split == False):\n",
        "            dataset = []\n",
        "            labels = []\n",
        "            for f in os.listdir(train_data_location):\n",
        "                print(f)\n",
        "                if(f == 'trec_5000_train.txt'):\n",
        "                    # Subjective Data\n",
        "                    with open(train_data_location + f, encoding = \"ISO-8859-1\") as subj_file:\n",
        "                        for line in subj_file:\n",
        "                            split_line = line.split(':')\n",
        "                            ques_class = split_line[0]\n",
        "                            question = split_line[1]\n",
        "                            pattern = \"[^a-zA-Z.' ]\"\n",
        "                            cleaned_line = re.sub(pattern,' ',question)\n",
        "                            cleaned_line = cleaned_line.lower()\n",
        "                            dataset.append(cleaned_line)\n",
        "                            if(ques_class == 'NUM'):\n",
        "                                labels.append(0)\n",
        "                            elif(ques_class == 'DESC'):\n",
        "                                labels.append(1)\n",
        "                            elif(ques_class == 'ENTY'):\n",
        "                                labels.append(2)\n",
        "                            elif(ques_class == 'HUM'):\n",
        "                                labels.append(3)\n",
        "                            elif(ques_class == 'ABBR'):\n",
        "                                labels.append(4)\n",
        "                            elif(ques_class == 'LOC'):\n",
        "                                labels.append(5)\n",
        "                elif(f == 'trec_test.txt'):\n",
        "                    # Objective Data\n",
        "                    with open(train_data_location + f, encoding = \"ISO-8859-1\") as obj_file:\n",
        "                        for line in obj_file:\n",
        "                            split_line = line.split(': ')\n",
        "                            ques_class = split_line[0]\n",
        "                            question = split_line[1]\n",
        "                            pattern = \"[^a-zA-Z.' ]\"\n",
        "                            cleaned_line = re.sub(pattern,' ',question)\n",
        "                            cleaned_line = cleaned_line.lower()\n",
        "                            dataset.append(cleaned_line)\n",
        "                            if(ques_class == 'NUM'):\n",
        "                                labels.append(0)\n",
        "                            elif(ques_class == 'DESC'):\n",
        "                                labels.append(1)\n",
        "                            elif(ques_class == 'ENTY'):\n",
        "                                labels.append(2)\n",
        "                            elif(ques_class == 'HUM'):\n",
        "                                labels.append(3)\n",
        "                            elif(ques_class == 'ABBR'):\n",
        "                                labels.append(4)\n",
        "                            elif(ques_class == 'LOC'):\n",
        "                                labels.append(5)\n",
        "            return np.array(dataset), np.array(labels)\n",
        "        elif(use_default_split==True):\n",
        "            x_train = []\n",
        "            x_test = []\n",
        "            y_train = []\n",
        "            y_test = []\n",
        "            for f in os.listdir(train_data_location):\n",
        "                print(f)\n",
        "                if(f == 'trec_5000_train.txt'):\n",
        "                    # Subjective Data\n",
        "                    with open(train_data_location + f, encoding = \"ISO-8859-1\") as subj_file:\n",
        "                        for line in subj_file:\n",
        "                            split_line = line.split(':')#\n",
        "                            ques_class = split_line[0]\n",
        "                            question = line.split(' ',1)[1]#split_line[1]\n",
        "                            pattern = \"[^a-zA-Z.' ]\"\n",
        "                            cleaned_line = re.sub(pattern,' ',question)\n",
        "                            cleaned_line = cleaned_line.lower()\n",
        "                            x_train.append(cleaned_line)\n",
        "                            if(ques_class == 'NUM'):\n",
        "                                y_train.append(0)\n",
        "                            elif(ques_class == 'DESC'):\n",
        "                                y_train.append(1)\n",
        "                            elif(ques_class == 'ENTY'):\n",
        "                                y_train.append(2)\n",
        "                            elif(ques_class == 'HUM'):\n",
        "                                y_train.append(3)\n",
        "                            elif(ques_class == 'ABBR'):\n",
        "                                y_train.append(4)\n",
        "                            elif(ques_class == 'LOC'):\n",
        "                                y_train.append(5)\n",
        "                elif(f == 'trec_test.txt'):\n",
        "                    # Objective Data\n",
        "                    with open(train_data_location + f, encoding = \"ISO-8859-1\") as obj_file:\n",
        "                        for line in obj_file:\n",
        "                            split_line = line.split(':')#line.split(' ',1)\n",
        "                            ques_class = split_line[0]\n",
        "                            question = line.split(' ',1)[1]#split_line[1]\n",
        "                            pattern = \"[^a-zA-Z.' ]\"\n",
        "                            cleaned_line = re.sub(pattern,' ',question)\n",
        "                            cleaned_line = cleaned_line.lower()\n",
        "                            x_test.append(cleaned_line)\n",
        "                            if(ques_class == 'NUM'):\n",
        "                                y_test.append(0)\n",
        "                            elif(ques_class == 'DESC'):\n",
        "                                y_test.append(1)\n",
        "                            elif(ques_class == 'ENTY'):\n",
        "                                y_test.append(2)\n",
        "                            elif(ques_class == 'HUM'):\n",
        "                                y_test.append(3)\n",
        "                            elif(ques_class == 'ABBR'):\n",
        "                                y_test.append(4)\n",
        "                            elif(ques_class == 'LOC'):\n",
        "                                y_test.append(5)\n",
        "            return x_train, y_train, x_test, y_test\n",
        "          \n",
        "    def open_pretrained(self):\n",
        "        \"\"\"Getting GloVe Embeddings to be used for embedding\n",
        "        layer. Corresponding words to be feature hashed for look\n",
        "        up.\n",
        "        Returns\n",
        "            NumPy Tensor of shape (300,)\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        glove_w2emb = {}\n",
        "        glove_embeddings_file = open(os.path.join('/content/drive/My Drive/College/Undergraduate Research/SkillEvaluation/','glove.6B.'+str(self.cfg['embedding_dim'])+'d.txt'))\n",
        "        \n",
        "        # -- Padding --\n",
        "        glove_w2emb['_pad'] = (0, None)\n",
        "        \n",
        "        # -- OOV Words --\n",
        "        unk_words = np.random.rand(self.cfg['embedding_dim'],)\n",
        "        glove_w2emb['_unk'] = (1, unk_words)\n",
        "        embeddings.append(unk_words)\n",
        "        \n",
        "        idx = 2\n",
        "        for line in tqdm(glove_embeddings_file):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            glove_w2emb[word] = (idx , coefs)\n",
        "            embeddings.append(coefs)\n",
        "            idx+=1\n",
        "        glove_embeddings_file.close()\n",
        "        return glove_w2emb, embeddings\n",
        "    \n",
        "    def open_bpe_vectors(self):\n",
        "        en_model = BPEmb(lang='en',vs=200000,dim=300)\n",
        "        return en_model, en_model.vectors\n",
        "        \n",
        "    \n",
        "    def build_embedding_table(self, mapping):\n",
        "        table = np.zeros((len(mapping), self.cfg['embedding_dim']))\n",
        "        for word, value in mapping.items():\n",
        "            if(value[1] is not None):\n",
        "                table[value[0]] = value[1]\n",
        "        return table\n",
        "    \n",
        "    \n",
        "    def split_data(self,examples,labels):\n",
        "        if(self.use_default_split == False):\n",
        "            cfg_split_ratio = self.cfg['train_test_split_ratio']\n",
        "            x_train, x_test, y_train, y_test = train_test_split(self.examples, self.labels, test_size=cfg_split_ratio, random_state=1000)\n",
        "            return x_train,x_test,y_train,y_test\n",
        "\n",
        "    def create_dataloaders(self,train_data,test_data):\n",
        "        x_train = train_data[0]\n",
        "        y_train = train_data[1]\n",
        "\n",
        "        x_test = test_data[0]\n",
        "        y_test = test_data[1]\n",
        "\n",
        "        if(str(device) == 'cuda'):\n",
        "            x_train = torch.tensor(x_train).cuda()\n",
        "            y_train = torch.tensor(y_train,dtype=torch.long).cuda()\n",
        "            x_test = torch.tensor(x_test).cuda()\n",
        "            y_test = torch.tensor(y_test,dtype=torch.long).cuda()\n",
        "        else:\n",
        "            x_train = torch.tensor(x_train)\n",
        "            y_train = torch.tensor(y_train,dtype=torch.long)\n",
        "            x_test = torch.tensor(x_test)\n",
        "            y_test = torch.tensor(y_test,dtype=torch.long)\n",
        "\n",
        "        train_dataloader = self.create_dataloader(features=x_train, labels=y_train)\n",
        "        test_dataloader = self.create_dataloader(features=x_test, labels=y_test)\n",
        "        return train_dataloader,test_dataloader\n",
        "\n",
        "    \n",
        "    def build_model(self, embeddings):\n",
        "        return ShallowCNN(embeddings)\n",
        "    \n",
        "    def train(self,train_data):\n",
        "\n",
        "        epochs = 100 # self.cfg['epochs']\n",
        "        learning_rate = 0.0001 #self.cfg['learning_rate'])\n",
        "        \n",
        "        # -- Create Model --\n",
        "        self.model = self.build_model(torch.tensor(self.embeddings))\n",
        "        print(self.model)\n",
        "\n",
        "        # -- Model to CUDA GPU --\n",
        "        if( str(device) == 'cuda'):\n",
        "            print('Sending model to',torch.cuda.get_device_name(device),' GPU')\n",
        "            #model = model.cuda()\n",
        "            self.model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(),lr=learning_rate)\n",
        "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
        "        #                                             step_size=50,\n",
        "        #                                             gamma=0.1)\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                               mode='min', \n",
        "                                                               factor=0.1, \n",
        "                                                               patience=5, \n",
        "                                                               verbose=True, \n",
        "                                                               threshold=0.0001, \n",
        "                                                               threshold_mode='rel', \n",
        "                                                               cooldown=0, \n",
        "                                                               min_lr=0, \n",
        "                                                               eps=1e-08)\n",
        "        #loss_function = torch.nn.CrossEntropyLoss()\n",
        "        loss_function = torch.nn.NLLLoss()\n",
        "        losses = []\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            loss = 0\n",
        "            for i , (examples, labels) in enumerate(train_data):\n",
        "                labels_n = labels.cpu().numpy()\n",
        "                labels_idx = np.argwhere(labels_n >0)\n",
        "                labels_idx = labels_idx.T\n",
        "                labels_idx = np.delete(labels_idx,0,0).T\n",
        "                labels_idx = np.squeeze(labels_idx,1)\n",
        "                labels_idx = torch.tensor(labels_idx,dtype=torch.int)\n",
        "                #print(labels_idx)\n",
        "\n",
        "                # Transfer to GPU\n",
        "                if(str(device) == 'cuda'):\n",
        "                    examples = examples.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    labels_idx = labels_idx.to(device)\n",
        "                \n",
        "                self.model.zero_grad()\n",
        "\n",
        "                predictions = self.model(examples.long())\n",
        "                loss = loss_function(predictions,labels_idx.long())\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                #break\n",
        "            scheduler.step(total_loss) \n",
        "            losses.append(total_loss)\n",
        "            #break\n",
        "            print('Epoch {} ----> loss={}'.format(epoch,total_loss))\n",
        "            #print('Epoch {} Learning_Rate{} ----> loss={}'.format(epoch,scheduler.get_lr(),total_loss))\n",
        "            print('==========================================================')\n",
        "        return self.model, loss_function, losses\n",
        "    \n",
        "    def test_validate(self,debug=False,model=None,test_data=[],loss_fn=None):\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        all_predictions = []\n",
        "        for idx,(examples, labels) in enumerate(test_data):\n",
        "\n",
        "            labels_n = labels.cpu().numpy()\n",
        "            labels_idx = np.argwhere(labels_n >0)\n",
        "            labels_idx = labels_idx.T\n",
        "            labels_idx = np.delete(labels_idx,0,0).T\n",
        "            labels_idx = np.squeeze(labels_idx,1)\n",
        "            labels_idx = torch.tensor(labels_idx,dtype=torch.int)\n",
        "            if(str(device) == 'cuda'):\n",
        "                examples = examples.to(device)\n",
        "                labels = labels.to(device)\n",
        "                labels_idx = labels_idx.to(device)\n",
        "\n",
        "            outputs = self.model.forward(examples.long())\n",
        "\n",
        "            preds = []\n",
        "            for pred in outputs:\n",
        "                #preds.append((torch.max(pred).detach(),np.argmax(pred.cpu().detach().numpy())))\n",
        "                preds.append(np.argmax(pred.cpu().detach().numpy()))\n",
        "            preds = torch.tensor(preds,dtype=torch.int).to(device)\n",
        "            \n",
        "            all_predictions.append(outputs)\n",
        "            loss = loss_fn(outputs, labels_idx.long())\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            correct += (preds == labels_idx).sum() \n",
        "            # print('correct = ',correct)\n",
        "            #accuracy = correct.float()/64 * 100\n",
        "\n",
        "            if(debug):\n",
        "                for ex,label,label_idx,pred,pred_idx in zip(examples,labels,labels_idx,outputs,preds):\n",
        "                    print('{}: actual = {} ---> pred = {}'.format(idx,label_idx.item(),pred_idx.item()))\n",
        "       \n",
        "        accuracy = correct.float()/500 * 100\n",
        "        return test_loss, accuracy, all_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Id0YMrp2-n",
        "colab_type": "text"
      },
      "source": [
        "## Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ygmqWrupqW",
        "colab_type": "code",
        "outputId": "53df0107-950a-4fcf-f493-80d92e02aa27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "train_op = Trainer()\n",
        "print(train_op.x_train)\n",
        "print(train_op.y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trec_5000_train.txt\n",
            "trec_test.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train data size: x_train = { (5452, 150) } -- y_train = { (5452, 6) }\n",
            "Test data size: x_test = { (500, 150) } -- y_test = { (500, 6) }\n",
            "-- Batch size  8 --\n",
            "-- Batch size  8 --\n",
            "[[    0     0     0 ...   637  3910  3519]\n",
            " [    0     0     0 ...  1259 65381 16279]\n",
            " [    0     0     0 ...   881  1550  2828]\n",
            " ...\n",
            " [    0     0     0 ...     7  4318  2185]\n",
            " [    0     0     0 ...  4318    72 12189]\n",
            " [    0     0     0 ...   483    26  2008]]\n",
            "[[0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4FcIQK4hNqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cef12aff-420e-47f8-c505-d472ce2b1a74"
      },
      "source": [
        "model,criterion,losses = train_op.train(train_data=train_op.train_dataloader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ShallowCNN(\n",
            "  (pretrained_embedding_table): Embedding(200000, 300)\n",
            "  (conv1): Conv1d(150, 400, kernel_size=(4,), stride=(1,))\n",
            "  (mp1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=118400, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
            ")\n",
            "Sending model to Tesla K80  GPU\n",
            "Epoch 0 ----> loss=977.0213705301285\n",
            "==========================================================\n",
            "Epoch 1 ----> loss=703.1730970144272\n",
            "==========================================================\n",
            "Epoch 2 ----> loss=541.4128501713276\n",
            "==========================================================\n",
            "Epoch 3 ----> loss=445.7710072696209\n",
            "==========================================================\n",
            "Epoch 4 ----> loss=375.3731412887573\n",
            "==========================================================\n",
            "Epoch 5 ----> loss=314.9521656036377\n",
            "==========================================================\n",
            "Epoch 6 ----> loss=246.72413147985935\n",
            "==========================================================\n",
            "Epoch 7 ----> loss=195.80047276616096\n",
            "==========================================================\n",
            "Epoch 8 ----> loss=166.50006020069122\n",
            "==========================================================\n",
            "Epoch 9 ----> loss=132.19839715957642\n",
            "==========================================================\n",
            "Epoch 10 ----> loss=102.18750877678394\n",
            "==========================================================\n",
            "Epoch 11 ----> loss=92.44186682999134\n",
            "==========================================================\n",
            "Epoch 12 ----> loss=56.429961785674095\n",
            "==========================================================\n",
            "Epoch 13 ----> loss=50.2213923484087\n",
            "==========================================================\n",
            "Epoch 14 ----> loss=55.23297522962093\n",
            "==========================================================\n",
            "Epoch 15 ----> loss=46.25152149796486\n",
            "==========================================================\n",
            "Epoch 16 ----> loss=33.83703897893429\n",
            "==========================================================\n",
            "Epoch 17 ----> loss=55.77019959688187\n",
            "==========================================================\n",
            "Epoch 18 ----> loss=16.841881155967712\n",
            "==========================================================\n",
            "Epoch 19 ----> loss=16.344324737787247\n",
            "==========================================================\n",
            "Epoch 20 ----> loss=46.53845965862274\n",
            "==========================================================\n",
            "Epoch 21 ----> loss=15.917126327753067\n",
            "==========================================================\n",
            "Epoch 22 ----> loss=8.562744855880737\n",
            "==========================================================\n",
            "Epoch 23 ----> loss=68.2472745925188\n",
            "==========================================================\n",
            "Epoch 24 ----> loss=6.9749467968940735\n",
            "==========================================================\n",
            "Epoch 25 ----> loss=7.757587194442749\n",
            "==========================================================\n",
            "Epoch 26 ----> loss=50.21243220567703\n",
            "==========================================================\n",
            "Epoch 27 ----> loss=4.983200818300247\n",
            "==========================================================\n",
            "Epoch 28 ----> loss=30.756179749965668\n",
            "==========================================================\n",
            "Epoch 29 ----> loss=8.714956402778625\n",
            "==========================================================\n",
            "Epoch 30 ----> loss=7.203914850950241\n",
            "==========================================================\n",
            "Epoch 31 ----> loss=41.886438965797424\n",
            "==========================================================\n",
            "Epoch 32 ----> loss=3.6551693379879\n",
            "==========================================================\n",
            "Epoch 33 ----> loss=26.871995270252228\n",
            "==========================================================\n",
            "Epoch 34 ----> loss=4.576034873723984\n",
            "==========================================================\n",
            "Epoch 35 ----> loss=38.92694813013077\n",
            "==========================================================\n",
            "Epoch 36 ----> loss=4.904156133532524\n",
            "==========================================================\n",
            "Epoch 37 ----> loss=29.350135296583176\n",
            "==========================================================\n",
            "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 38 ----> loss=30.693181231617928\n",
            "==========================================================\n",
            "Epoch 39 ----> loss=1.7906052768230438\n",
            "==========================================================\n",
            "Epoch 40 ----> loss=1.3460314273834229\n",
            "==========================================================\n",
            "Epoch 41 ----> loss=1.2481608092784882\n",
            "==========================================================\n",
            "Epoch 42 ----> loss=1.279114544391632\n",
            "==========================================================\n",
            "Epoch 43 ----> loss=1.1404149532318115\n",
            "==========================================================\n",
            "Epoch 44 ----> loss=0.9312571883201599\n",
            "==========================================================\n",
            "Epoch 45 ----> loss=0.9323514699935913\n",
            "==========================================================\n",
            "Epoch 46 ----> loss=0.9632497131824493\n",
            "==========================================================\n",
            "Epoch 47 ----> loss=1.2917922139167786\n",
            "==========================================================\n",
            "Epoch 48 ----> loss=1.0451267659664154\n",
            "==========================================================\n",
            "Epoch 49 ----> loss=0.6337660551071167\n",
            "==========================================================\n",
            "Epoch 50 ----> loss=0.7435169517993927\n",
            "==========================================================\n",
            "Epoch 51 ----> loss=0.7581183612346649\n",
            "==========================================================\n",
            "Epoch 52 ----> loss=0.581877589225769\n",
            "==========================================================\n",
            "Epoch 53 ----> loss=0.7314835786819458\n",
            "==========================================================\n",
            "Epoch 54 ----> loss=0.9858750402927399\n",
            "==========================================================\n",
            "Epoch 55 ----> loss=0.3797715902328491\n",
            "==========================================================\n",
            "Epoch 56 ----> loss=0.9946101903915405\n",
            "==========================================================\n",
            "Epoch 57 ----> loss=0.4264715909957886\n",
            "==========================================================\n",
            "Epoch 58 ----> loss=0.9702172875404358\n",
            "==========================================================\n",
            "Epoch 59 ----> loss=0.7757245302200317\n",
            "==========================================================\n",
            "Epoch 60 ----> loss=0.3381504416465759\n",
            "==========================================================\n",
            "Epoch 61 ----> loss=0.9889169335365295\n",
            "==========================================================\n",
            "Epoch 62 ----> loss=0.31168124079704285\n",
            "==========================================================\n",
            "Epoch 63 ----> loss=0.768701046705246\n",
            "==========================================================\n",
            "Epoch 64 ----> loss=0.2891715168952942\n",
            "==========================================================\n",
            "Epoch 65 ----> loss=0.3994775116443634\n",
            "==========================================================\n",
            "Epoch 66 ----> loss=0.8894568681716919\n",
            "==========================================================\n",
            "Epoch 67 ----> loss=0.32688766717910767\n",
            "==========================================================\n",
            "Epoch 68 ----> loss=0.8104921579360962\n",
            "==========================================================\n",
            "Epoch 69 ----> loss=0.6549609899520874\n",
            "==========================================================\n",
            "Epoch 70 ----> loss=0.24118143320083618\n",
            "==========================================================\n",
            "Epoch 71 ----> loss=0.36219459772109985\n",
            "==========================================================\n",
            "Epoch 72 ----> loss=0.44945085048675537\n",
            "==========================================================\n",
            "Epoch 73 ----> loss=0.7490160465240479\n",
            "==========================================================\n",
            "Epoch 74 ----> loss=0.27773797512054443\n",
            "==========================================================\n",
            "Epoch 75 ----> loss=0.28797435760498047\n",
            "==========================================================\n",
            "Epoch    76: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Epoch 76 ----> loss=0.2791803479194641\n",
            "==========================================================\n",
            "Epoch 77 ----> loss=0.43735653162002563\n",
            "==========================================================\n",
            "Epoch 78 ----> loss=0.2926926612854004\n",
            "==========================================================\n",
            "Epoch 79 ----> loss=0.18067800998687744\n",
            "==========================================================\n",
            "Epoch 80 ----> loss=0.11412268877029419\n",
            "==========================================================\n",
            "Epoch 81 ----> loss=0.08721953630447388\n",
            "==========================================================\n",
            "Epoch 82 ----> loss=0.07504540681838989\n",
            "==========================================================\n",
            "Epoch 83 ----> loss=0.06759756803512573\n",
            "==========================================================\n",
            "Epoch 84 ----> loss=0.06295204162597656\n",
            "==========================================================\n",
            "Epoch 85 ----> loss=0.06326770782470703\n",
            "==========================================================\n",
            "Epoch 86 ----> loss=0.06116318702697754\n",
            "==========================================================\n",
            "Epoch 87 ----> loss=0.06039661169052124\n",
            "==========================================================\n",
            "Epoch 88 ----> loss=0.060293495655059814\n",
            "==========================================================\n",
            "Epoch 89 ----> loss=0.059180378913879395\n",
            "==========================================================\n",
            "Epoch 90 ----> loss=0.05773097276687622\n",
            "==========================================================\n",
            "Epoch 91 ----> loss=0.055444955825805664\n",
            "==========================================================\n",
            "Epoch 92 ----> loss=0.0554545521736145\n",
            "==========================================================\n",
            "Epoch 93 ----> loss=0.05480349063873291\n",
            "==========================================================\n",
            "Epoch 94 ----> loss=0.052006661891937256\n",
            "==========================================================\n",
            "Epoch 95 ----> loss=0.052403271198272705\n",
            "==========================================================\n",
            "Epoch 96 ----> loss=0.052268385887145996\n",
            "==========================================================\n",
            "Epoch 97 ----> loss=0.05053967237472534\n",
            "==========================================================\n",
            "Epoch 98 ----> loss=0.049961626529693604\n",
            "==========================================================\n",
            "Epoch 99 ----> loss=0.04987692832946777\n",
            "==========================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE9woWj42PAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f65ed337-d8a5-4591-c56b-5766c5994c52"
      },
      "source": [
        "test_loss,acc,preds = train_op.test_validate(debug=True,model=train_op.model,test_data=train_op.test_dataloader,loss_fn=criterion)\n",
        "print('Test Accuracy = {}%'.format(acc))\n",
        "# 50 Epochs lr=0.0001 - 75% \n",
        "# 100 Epochs lr=0.0001 - 81% & 78.125\n",
        "# 200 Epoch lr = 0.00001 - 76.5625\n",
        "# 5000 Epoch lr = 0.000001 - 60.937%"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: actual = 1 ---> pred = 1\n",
            "0: actual = 3 ---> pred = 3\n",
            "0: actual = 1 ---> pred = 1\n",
            "0: actual = 0 ---> pred = 0\n",
            "0: actual = 0 ---> pred = 0\n",
            "0: actual = 3 ---> pred = 3\n",
            "0: actual = 0 ---> pred = 0\n",
            "0: actual = 2 ---> pred = 1\n",
            "1: actual = 5 ---> pred = 5\n",
            "1: actual = 5 ---> pred = 5\n",
            "1: actual = 3 ---> pred = 3\n",
            "1: actual = 2 ---> pred = 5\n",
            "1: actual = 0 ---> pred = 0\n",
            "1: actual = 2 ---> pred = 2\n",
            "1: actual = 3 ---> pred = 3\n",
            "1: actual = 2 ---> pred = 3\n",
            "2: actual = 0 ---> pred = 1\n",
            "2: actual = 5 ---> pred = 5\n",
            "2: actual = 1 ---> pred = 1\n",
            "2: actual = 1 ---> pred = 1\n",
            "2: actual = 2 ---> pred = 2\n",
            "2: actual = 0 ---> pred = 0\n",
            "2: actual = 1 ---> pred = 1\n",
            "2: actual = 3 ---> pred = 2\n",
            "3: actual = 5 ---> pred = 5\n",
            "3: actual = 2 ---> pred = 5\n",
            "3: actual = 0 ---> pred = 2\n",
            "3: actual = 1 ---> pred = 1\n",
            "3: actual = 0 ---> pred = 1\n",
            "3: actual = 2 ---> pred = 2\n",
            "3: actual = 2 ---> pred = 2\n",
            "3: actual = 2 ---> pred = 2\n",
            "4: actual = 2 ---> pred = 1\n",
            "4: actual = 0 ---> pred = 0\n",
            "4: actual = 2 ---> pred = 2\n",
            "4: actual = 2 ---> pred = 2\n",
            "4: actual = 3 ---> pred = 3\n",
            "4: actual = 1 ---> pred = 1\n",
            "4: actual = 4 ---> pred = 4\n",
            "4: actual = 1 ---> pred = 1\n",
            "5: actual = 2 ---> pred = 0\n",
            "5: actual = 4 ---> pred = 1\n",
            "5: actual = 2 ---> pred = 2\n",
            "5: actual = 5 ---> pred = 5\n",
            "5: actual = 1 ---> pred = 1\n",
            "5: actual = 2 ---> pred = 2\n",
            "5: actual = 2 ---> pred = 5\n",
            "5: actual = 0 ---> pred = 0\n",
            "6: actual = 1 ---> pred = 1\n",
            "6: actual = 5 ---> pred = 1\n",
            "6: actual = 0 ---> pred = 3\n",
            "6: actual = 3 ---> pred = 3\n",
            "6: actual = 3 ---> pred = 3\n",
            "6: actual = 0 ---> pred = 0\n",
            "6: actual = 0 ---> pred = 1\n",
            "6: actual = 0 ---> pred = 0\n",
            "7: actual = 0 ---> pred = 0\n",
            "7: actual = 1 ---> pred = 1\n",
            "7: actual = 0 ---> pred = 0\n",
            "7: actual = 1 ---> pred = 1\n",
            "7: actual = 0 ---> pred = 0\n",
            "7: actual = 1 ---> pred = 1\n",
            "7: actual = 3 ---> pred = 5\n",
            "7: actual = 0 ---> pred = 3\n",
            "8: actual = 5 ---> pred = 5\n",
            "8: actual = 5 ---> pred = 5\n",
            "8: actual = 0 ---> pred = 5\n",
            "8: actual = 1 ---> pred = 1\n",
            "8: actual = 3 ---> pred = 3\n",
            "8: actual = 5 ---> pred = 5\n",
            "8: actual = 2 ---> pred = 2\n",
            "8: actual = 5 ---> pred = 1\n",
            "9: actual = 5 ---> pred = 2\n",
            "9: actual = 0 ---> pred = 0\n",
            "9: actual = 5 ---> pred = 5\n",
            "9: actual = 1 ---> pred = 1\n",
            "9: actual = 1 ---> pred = 1\n",
            "9: actual = 0 ---> pred = 0\n",
            "9: actual = 3 ---> pred = 3\n",
            "9: actual = 1 ---> pred = 1\n",
            "10: actual = 0 ---> pred = 0\n",
            "10: actual = 3 ---> pred = 3\n",
            "10: actual = 2 ---> pred = 2\n",
            "10: actual = 0 ---> pred = 0\n",
            "10: actual = 1 ---> pred = 2\n",
            "10: actual = 0 ---> pred = 0\n",
            "10: actual = 5 ---> pred = 5\n",
            "10: actual = 0 ---> pred = 0\n",
            "11: actual = 2 ---> pred = 2\n",
            "11: actual = 3 ---> pred = 3\n",
            "11: actual = 2 ---> pred = 0\n",
            "11: actual = 0 ---> pred = 0\n",
            "11: actual = 1 ---> pred = 1\n",
            "11: actual = 1 ---> pred = 1\n",
            "11: actual = 3 ---> pred = 3\n",
            "11: actual = 3 ---> pred = 3\n",
            "12: actual = 0 ---> pred = 0\n",
            "12: actual = 2 ---> pred = 2\n",
            "12: actual = 5 ---> pred = 5\n",
            "12: actual = 2 ---> pred = 2\n",
            "12: actual = 2 ---> pred = 1\n",
            "12: actual = 1 ---> pred = 1\n",
            "12: actual = 5 ---> pred = 5\n",
            "12: actual = 0 ---> pred = 0\n",
            "13: actual = 1 ---> pred = 1\n",
            "13: actual = 0 ---> pred = 1\n",
            "13: actual = 5 ---> pred = 5\n",
            "13: actual = 0 ---> pred = 0\n",
            "13: actual = 1 ---> pred = 1\n",
            "13: actual = 1 ---> pred = 1\n",
            "13: actual = 5 ---> pred = 5\n",
            "13: actual = 5 ---> pred = 5\n",
            "14: actual = 2 ---> pred = 2\n",
            "14: actual = 1 ---> pred = 1\n",
            "14: actual = 2 ---> pred = 2\n",
            "14: actual = 0 ---> pred = 1\n",
            "14: actual = 3 ---> pred = 3\n",
            "14: actual = 1 ---> pred = 1\n",
            "14: actual = 5 ---> pred = 5\n",
            "14: actual = 1 ---> pred = 1\n",
            "15: actual = 2 ---> pred = 0\n",
            "15: actual = 0 ---> pred = 0\n",
            "15: actual = 1 ---> pred = 1\n",
            "15: actual = 5 ---> pred = 5\n",
            "15: actual = 1 ---> pred = 1\n",
            "15: actual = 1 ---> pred = 1\n",
            "15: actual = 5 ---> pred = 5\n",
            "15: actual = 2 ---> pred = 2\n",
            "16: actual = 3 ---> pred = 3\n",
            "16: actual = 2 ---> pred = 1\n",
            "16: actual = 4 ---> pred = 4\n",
            "16: actual = 3 ---> pred = 3\n",
            "16: actual = 2 ---> pred = 2\n",
            "16: actual = 5 ---> pred = 5\n",
            "16: actual = 5 ---> pred = 5\n",
            "16: actual = 0 ---> pred = 0\n",
            "17: actual = 5 ---> pred = 5\n",
            "17: actual = 1 ---> pred = 1\n",
            "17: actual = 2 ---> pred = 2\n",
            "17: actual = 1 ---> pred = 1\n",
            "17: actual = 0 ---> pred = 0\n",
            "17: actual = 0 ---> pred = 0\n",
            "17: actual = 3 ---> pred = 3\n",
            "17: actual = 2 ---> pred = 2\n",
            "18: actual = 2 ---> pred = 2\n",
            "18: actual = 5 ---> pred = 5\n",
            "18: actual = 5 ---> pred = 5\n",
            "18: actual = 1 ---> pred = 1\n",
            "18: actual = 1 ---> pred = 1\n",
            "18: actual = 1 ---> pred = 1\n",
            "18: actual = 1 ---> pred = 1\n",
            "18: actual = 0 ---> pred = 0\n",
            "19: actual = 0 ---> pred = 0\n",
            "19: actual = 5 ---> pred = 5\n",
            "19: actual = 3 ---> pred = 3\n",
            "19: actual = 1 ---> pred = 1\n",
            "19: actual = 1 ---> pred = 1\n",
            "19: actual = 3 ---> pred = 3\n",
            "19: actual = 1 ---> pred = 1\n",
            "19: actual = 0 ---> pred = 0\n",
            "20: actual = 1 ---> pred = 1\n",
            "20: actual = 3 ---> pred = 2\n",
            "20: actual = 3 ---> pred = 3\n",
            "20: actual = 4 ---> pred = 4\n",
            "20: actual = 1 ---> pred = 1\n",
            "20: actual = 1 ---> pred = 1\n",
            "20: actual = 0 ---> pred = 0\n",
            "20: actual = 0 ---> pred = 0\n",
            "21: actual = 1 ---> pred = 1\n",
            "21: actual = 5 ---> pred = 2\n",
            "21: actual = 5 ---> pred = 5\n",
            "21: actual = 1 ---> pred = 1\n",
            "21: actual = 3 ---> pred = 3\n",
            "21: actual = 1 ---> pred = 1\n",
            "21: actual = 3 ---> pred = 3\n",
            "21: actual = 0 ---> pred = 0\n",
            "22: actual = 0 ---> pred = 0\n",
            "22: actual = 0 ---> pred = 0\n",
            "22: actual = 0 ---> pred = 0\n",
            "22: actual = 1 ---> pred = 1\n",
            "22: actual = 2 ---> pred = 2\n",
            "22: actual = 5 ---> pred = 5\n",
            "22: actual = 1 ---> pred = 1\n",
            "22: actual = 1 ---> pred = 1\n",
            "23: actual = 2 ---> pred = 1\n",
            "23: actual = 2 ---> pred = 2\n",
            "23: actual = 0 ---> pred = 1\n",
            "23: actual = 3 ---> pred = 5\n",
            "23: actual = 1 ---> pred = 1\n",
            "23: actual = 2 ---> pred = 2\n",
            "23: actual = 0 ---> pred = 0\n",
            "23: actual = 0 ---> pred = 0\n",
            "24: actual = 2 ---> pred = 2\n",
            "24: actual = 2 ---> pred = 2\n",
            "24: actual = 2 ---> pred = 2\n",
            "24: actual = 1 ---> pred = 1\n",
            "24: actual = 3 ---> pred = 3\n",
            "24: actual = 1 ---> pred = 1\n",
            "24: actual = 3 ---> pred = 3\n",
            "24: actual = 3 ---> pred = 3\n",
            "25: actual = 1 ---> pred = 2\n",
            "25: actual = 1 ---> pred = 2\n",
            "25: actual = 2 ---> pred = 1\n",
            "25: actual = 5 ---> pred = 5\n",
            "25: actual = 1 ---> pred = 1\n",
            "25: actual = 2 ---> pred = 2\n",
            "25: actual = 1 ---> pred = 1\n",
            "25: actual = 2 ---> pred = 2\n",
            "26: actual = 5 ---> pred = 5\n",
            "26: actual = 1 ---> pred = 1\n",
            "26: actual = 5 ---> pred = 5\n",
            "26: actual = 1 ---> pred = 1\n",
            "26: actual = 1 ---> pred = 1\n",
            "26: actual = 0 ---> pred = 0\n",
            "26: actual = 0 ---> pred = 0\n",
            "26: actual = 1 ---> pred = 0\n",
            "27: actual = 0 ---> pred = 0\n",
            "27: actual = 1 ---> pred = 1\n",
            "27: actual = 3 ---> pred = 3\n",
            "27: actual = 0 ---> pred = 0\n",
            "27: actual = 4 ---> pred = 4\n",
            "27: actual = 5 ---> pred = 5\n",
            "27: actual = 5 ---> pred = 5\n",
            "27: actual = 1 ---> pred = 1\n",
            "28: actual = 1 ---> pred = 1\n",
            "28: actual = 1 ---> pred = 1\n",
            "28: actual = 2 ---> pred = 2\n",
            "28: actual = 1 ---> pred = 1\n",
            "28: actual = 3 ---> pred = 2\n",
            "28: actual = 5 ---> pred = 5\n",
            "28: actual = 0 ---> pred = 0\n",
            "28: actual = 2 ---> pred = 2\n",
            "29: actual = 1 ---> pred = 1\n",
            "29: actual = 2 ---> pred = 2\n",
            "29: actual = 2 ---> pred = 2\n",
            "29: actual = 0 ---> pred = 0\n",
            "29: actual = 3 ---> pred = 3\n",
            "29: actual = 0 ---> pred = 0\n",
            "29: actual = 5 ---> pred = 5\n",
            "29: actual = 1 ---> pred = 1\n",
            "30: actual = 3 ---> pred = 3\n",
            "30: actual = 5 ---> pred = 5\n",
            "30: actual = 0 ---> pred = 1\n",
            "30: actual = 5 ---> pred = 2\n",
            "30: actual = 3 ---> pred = 3\n",
            "30: actual = 0 ---> pred = 0\n",
            "30: actual = 0 ---> pred = 0\n",
            "30: actual = 1 ---> pred = 1\n",
            "31: actual = 1 ---> pred = 1\n",
            "31: actual = 3 ---> pred = 3\n",
            "31: actual = 5 ---> pred = 5\n",
            "31: actual = 5 ---> pred = 5\n",
            "31: actual = 4 ---> pred = 4\n",
            "31: actual = 1 ---> pred = 1\n",
            "31: actual = 0 ---> pred = 0\n",
            "31: actual = 5 ---> pred = 5\n",
            "32: actual = 2 ---> pred = 2\n",
            "32: actual = 0 ---> pred = 0\n",
            "32: actual = 1 ---> pred = 1\n",
            "32: actual = 5 ---> pred = 5\n",
            "32: actual = 0 ---> pred = 0\n",
            "32: actual = 0 ---> pred = 0\n",
            "32: actual = 3 ---> pred = 3\n",
            "32: actual = 2 ---> pred = 2\n",
            "33: actual = 0 ---> pred = 0\n",
            "33: actual = 0 ---> pred = 0\n",
            "33: actual = 2 ---> pred = 2\n",
            "33: actual = 5 ---> pred = 5\n",
            "33: actual = 1 ---> pred = 1\n",
            "33: actual = 1 ---> pred = 1\n",
            "33: actual = 0 ---> pred = 0\n",
            "33: actual = 3 ---> pred = 3\n",
            "34: actual = 3 ---> pred = 3\n",
            "34: actual = 2 ---> pred = 2\n",
            "34: actual = 1 ---> pred = 1\n",
            "34: actual = 1 ---> pred = 1\n",
            "34: actual = 3 ---> pred = 3\n",
            "34: actual = 0 ---> pred = 1\n",
            "34: actual = 2 ---> pred = 0\n",
            "34: actual = 0 ---> pred = 0\n",
            "35: actual = 2 ---> pred = 2\n",
            "35: actual = 1 ---> pred = 1\n",
            "35: actual = 1 ---> pred = 1\n",
            "35: actual = 3 ---> pred = 1\n",
            "35: actual = 5 ---> pred = 5\n",
            "35: actual = 0 ---> pred = 0\n",
            "35: actual = 0 ---> pred = 0\n",
            "35: actual = 2 ---> pred = 2\n",
            "36: actual = 1 ---> pred = 1\n",
            "36: actual = 5 ---> pred = 5\n",
            "36: actual = 0 ---> pred = 0\n",
            "36: actual = 3 ---> pred = 3\n",
            "36: actual = 3 ---> pred = 3\n",
            "36: actual = 0 ---> pred = 0\n",
            "36: actual = 0 ---> pred = 0\n",
            "36: actual = 1 ---> pred = 1\n",
            "37: actual = 1 ---> pred = 1\n",
            "37: actual = 1 ---> pred = 1\n",
            "37: actual = 5 ---> pred = 5\n",
            "37: actual = 1 ---> pred = 1\n",
            "37: actual = 5 ---> pred = 5\n",
            "37: actual = 5 ---> pred = 5\n",
            "37: actual = 5 ---> pred = 1\n",
            "37: actual = 0 ---> pred = 0\n",
            "38: actual = 1 ---> pred = 1\n",
            "38: actual = 2 ---> pred = 2\n",
            "38: actual = 0 ---> pred = 0\n",
            "38: actual = 5 ---> pred = 1\n",
            "38: actual = 3 ---> pred = 5\n",
            "38: actual = 1 ---> pred = 1\n",
            "38: actual = 1 ---> pred = 1\n",
            "38: actual = 2 ---> pred = 1\n",
            "39: actual = 1 ---> pred = 1\n",
            "39: actual = 1 ---> pred = 1\n",
            "39: actual = 2 ---> pred = 1\n",
            "39: actual = 5 ---> pred = 5\n",
            "39: actual = 0 ---> pred = 0\n",
            "39: actual = 2 ---> pred = 2\n",
            "39: actual = 0 ---> pred = 5\n",
            "39: actual = 1 ---> pred = 1\n",
            "40: actual = 2 ---> pred = 1\n",
            "40: actual = 2 ---> pred = 3\n",
            "40: actual = 1 ---> pred = 1\n",
            "40: actual = 2 ---> pred = 2\n",
            "40: actual = 2 ---> pred = 2\n",
            "40: actual = 1 ---> pred = 1\n",
            "40: actual = 1 ---> pred = 1\n",
            "40: actual = 0 ---> pred = 0\n",
            "41: actual = 3 ---> pred = 3\n",
            "41: actual = 2 ---> pred = 2\n",
            "41: actual = 5 ---> pred = 5\n",
            "41: actual = 1 ---> pred = 1\n",
            "41: actual = 0 ---> pred = 0\n",
            "41: actual = 2 ---> pred = 2\n",
            "41: actual = 3 ---> pred = 1\n",
            "41: actual = 2 ---> pred = 2\n",
            "42: actual = 1 ---> pred = 1\n",
            "42: actual = 2 ---> pred = 2\n",
            "42: actual = 2 ---> pred = 2\n",
            "42: actual = 5 ---> pred = 5\n",
            "42: actual = 1 ---> pred = 1\n",
            "42: actual = 0 ---> pred = 0\n",
            "42: actual = 0 ---> pred = 0\n",
            "42: actual = 1 ---> pred = 1\n",
            "43: actual = 0 ---> pred = 0\n",
            "43: actual = 1 ---> pred = 1\n",
            "43: actual = 0 ---> pred = 0\n",
            "43: actual = 5 ---> pred = 5\n",
            "43: actual = 1 ---> pred = 1\n",
            "43: actual = 4 ---> pred = 4\n",
            "43: actual = 3 ---> pred = 2\n",
            "43: actual = 3 ---> pred = 3\n",
            "44: actual = 2 ---> pred = 2\n",
            "44: actual = 1 ---> pred = 1\n",
            "44: actual = 1 ---> pred = 1\n",
            "44: actual = 5 ---> pred = 3\n",
            "44: actual = 1 ---> pred = 1\n",
            "44: actual = 0 ---> pred = 0\n",
            "44: actual = 0 ---> pred = 5\n",
            "44: actual = 1 ---> pred = 1\n",
            "45: actual = 5 ---> pred = 5\n",
            "45: actual = 1 ---> pred = 1\n",
            "45: actual = 5 ---> pred = 5\n",
            "45: actual = 0 ---> pred = 0\n",
            "45: actual = 0 ---> pred = 0\n",
            "45: actual = 2 ---> pred = 2\n",
            "45: actual = 0 ---> pred = 2\n",
            "45: actual = 3 ---> pred = 3\n",
            "46: actual = 1 ---> pred = 1\n",
            "46: actual = 2 ---> pred = 2\n",
            "46: actual = 0 ---> pred = 0\n",
            "46: actual = 3 ---> pred = 3\n",
            "46: actual = 2 ---> pred = 0\n",
            "46: actual = 5 ---> pred = 5\n",
            "46: actual = 5 ---> pred = 5\n",
            "46: actual = 1 ---> pred = 1\n",
            "47: actual = 2 ---> pred = 2\n",
            "47: actual = 0 ---> pred = 0\n",
            "47: actual = 5 ---> pred = 5\n",
            "47: actual = 5 ---> pred = 5\n",
            "47: actual = 1 ---> pred = 1\n",
            "47: actual = 1 ---> pred = 1\n",
            "47: actual = 2 ---> pred = 0\n",
            "47: actual = 3 ---> pred = 3\n",
            "48: actual = 3 ---> pred = 3\n",
            "48: actual = 2 ---> pred = 2\n",
            "48: actual = 0 ---> pred = 0\n",
            "48: actual = 1 ---> pred = 1\n",
            "48: actual = 1 ---> pred = 1\n",
            "48: actual = 5 ---> pred = 1\n",
            "48: actual = 1 ---> pred = 1\n",
            "48: actual = 3 ---> pred = 3\n",
            "49: actual = 5 ---> pred = 5\n",
            "49: actual = 5 ---> pred = 5\n",
            "49: actual = 5 ---> pred = 5\n",
            "49: actual = 2 ---> pred = 2\n",
            "49: actual = 1 ---> pred = 1\n",
            "49: actual = 5 ---> pred = 5\n",
            "49: actual = 1 ---> pred = 1\n",
            "49: actual = 3 ---> pred = 3\n",
            "50: actual = 5 ---> pred = 3\n",
            "50: actual = 1 ---> pred = 1\n",
            "50: actual = 1 ---> pred = 1\n",
            "50: actual = 1 ---> pred = 1\n",
            "50: actual = 5 ---> pred = 5\n",
            "50: actual = 0 ---> pred = 0\n",
            "50: actual = 2 ---> pred = 2\n",
            "50: actual = 1 ---> pred = 1\n",
            "51: actual = 0 ---> pred = 0\n",
            "51: actual = 0 ---> pred = 0\n",
            "51: actual = 0 ---> pred = 0\n",
            "51: actual = 0 ---> pred = 0\n",
            "51: actual = 2 ---> pred = 1\n",
            "51: actual = 1 ---> pred = 1\n",
            "51: actual = 5 ---> pred = 5\n",
            "51: actual = 5 ---> pred = 5\n",
            "52: actual = 2 ---> pred = 1\n",
            "52: actual = 1 ---> pred = 1\n",
            "52: actual = 1 ---> pred = 1\n",
            "52: actual = 0 ---> pred = 0\n",
            "52: actual = 4 ---> pred = 1\n",
            "52: actual = 0 ---> pred = 0\n",
            "52: actual = 5 ---> pred = 5\n",
            "52: actual = 2 ---> pred = 3\n",
            "53: actual = 2 ---> pred = 2\n",
            "53: actual = 0 ---> pred = 2\n",
            "53: actual = 1 ---> pred = 1\n",
            "53: actual = 1 ---> pred = 1\n",
            "53: actual = 1 ---> pred = 1\n",
            "53: actual = 2 ---> pred = 2\n",
            "53: actual = 2 ---> pred = 2\n",
            "53: actual = 2 ---> pred = 2\n",
            "54: actual = 5 ---> pred = 5\n",
            "54: actual = 5 ---> pred = 5\n",
            "54: actual = 3 ---> pred = 5\n",
            "54: actual = 1 ---> pred = 1\n",
            "54: actual = 0 ---> pred = 0\n",
            "54: actual = 5 ---> pred = 5\n",
            "54: actual = 3 ---> pred = 3\n",
            "54: actual = 0 ---> pred = 0\n",
            "55: actual = 0 ---> pred = 0\n",
            "55: actual = 2 ---> pred = 2\n",
            "55: actual = 1 ---> pred = 2\n",
            "55: actual = 1 ---> pred = 1\n",
            "55: actual = 0 ---> pred = 0\n",
            "55: actual = 5 ---> pred = 5\n",
            "55: actual = 0 ---> pred = 0\n",
            "55: actual = 5 ---> pred = 5\n",
            "56: actual = 1 ---> pred = 1\n",
            "56: actual = 2 ---> pred = 3\n",
            "56: actual = 5 ---> pred = 5\n",
            "56: actual = 2 ---> pred = 1\n",
            "56: actual = 0 ---> pred = 1\n",
            "56: actual = 2 ---> pred = 2\n",
            "56: actual = 0 ---> pred = 1\n",
            "56: actual = 2 ---> pred = 0\n",
            "57: actual = 0 ---> pred = 0\n",
            "57: actual = 3 ---> pred = 3\n",
            "57: actual = 2 ---> pred = 1\n",
            "57: actual = 1 ---> pred = 1\n",
            "57: actual = 3 ---> pred = 3\n",
            "57: actual = 1 ---> pred = 1\n",
            "57: actual = 3 ---> pred = 3\n",
            "57: actual = 0 ---> pred = 0\n",
            "58: actual = 5 ---> pred = 5\n",
            "58: actual = 1 ---> pred = 1\n",
            "58: actual = 3 ---> pred = 3\n",
            "58: actual = 0 ---> pred = 5\n",
            "58: actual = 3 ---> pred = 3\n",
            "58: actual = 3 ---> pred = 3\n",
            "58: actual = 3 ---> pred = 3\n",
            "58: actual = 2 ---> pred = 2\n",
            "59: actual = 1 ---> pred = 1\n",
            "59: actual = 0 ---> pred = 2\n",
            "59: actual = 1 ---> pred = 1\n",
            "59: actual = 1 ---> pred = 1\n",
            "59: actual = 3 ---> pred = 3\n",
            "59: actual = 1 ---> pred = 1\n",
            "59: actual = 5 ---> pred = 5\n",
            "59: actual = 1 ---> pred = 1\n",
            "60: actual = 2 ---> pred = 2\n",
            "60: actual = 1 ---> pred = 1\n",
            "60: actual = 1 ---> pred = 1\n",
            "60: actual = 1 ---> pred = 1\n",
            "60: actual = 0 ---> pred = 0\n",
            "60: actual = 3 ---> pred = 5\n",
            "60: actual = 0 ---> pred = 0\n",
            "60: actual = 0 ---> pred = 0\n",
            "61: actual = 0 ---> pred = 0\n",
            "61: actual = 0 ---> pred = 0\n",
            "61: actual = 5 ---> pred = 5\n",
            "61: actual = 1 ---> pred = 1\n",
            "61: actual = 1 ---> pred = 1\n",
            "61: actual = 2 ---> pred = 2\n",
            "61: actual = 1 ---> pred = 1\n",
            "61: actual = 4 ---> pred = 4\n",
            "62: actual = 2 ---> pred = 2\n",
            "62: actual = 2 ---> pred = 3\n",
            "62: actual = 3 ---> pred = 3\n",
            "62: actual = 2 ---> pred = 2\n",
            "Test Accuracy = 84.80000305175781%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlOFQu76wUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}